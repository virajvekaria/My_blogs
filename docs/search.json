[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blogs",
    "section": "",
    "text": "Siren\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nVannsh Jani and Viraj Vekaria\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nOptimization algorithms\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nViraj Vekaria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "code/sgd_momentum.html",
    "href": "code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nTealdsfhlKSDfa"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Optimization algorithms",
    "section": "",
    "text": "Test test"
  },
  {
    "objectID": "posts/post-with-code/sgd_momentum.html",
    "href": "posts/post-with-code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "docs/posts/SIREN/siren.html",
    "href": "docs/posts/SIREN/siren.html",
    "title": "Siren",
    "section": "",
    "text": "Implicit Neural representations using sinusoidal activation\n\n\nAudio compression\nThe reason we use the sine function as an activation function is because it is infinitely differentiable where as for a function like relu, the double derivative becomes zero. Let’s try regenerating an audio file by mapping coordinates in the grid with amplitudes\nThe following is the ground truth audio\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe are using a neural network with 3 hidden layers and each layer consisting of 256 neurons and we are using mean squared error loss function and optimizing it using Adam with a learning rate of 0.0001. We are training the neural network for a total of 1000 steps and displaying every 100th result. As can be seen in the following figures as the number of steps increases both the graphs converge.\nDescription of neural network\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=1, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.025424\nStep 100, Total loss 0.002972\nStep 200, Total loss 0.001123\nStep 300, Total loss 0.000861\nStep 400, Total loss 0.000702\nStep 500, Total loss 0.000586\nStep 600, Total loss 0.000535\nStep 700, Total loss 0.000538\nStep 800, Total loss 0.000665\nStep 900, Total loss 0.000362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph the coordinates of the grid are squeezed to a single dimension (x-axis) and the corresponding model output is shown on the y-axis.\nFollowing is the audio regenerated using a neural network with sinusoidal activation\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThere is a little bit of noise in the above regenerated audio signal but overall has a good performance\nLet’s try using a different activation function such as relu and how well it can perform to regenerate audio and compare it with sinusoidal activation\n\n\nStep 0, Total loss 0.032153\nStep 100, Total loss 0.032153\nStep 200, Total loss 0.032153\nStep 300, Total loss 0.032153\nStep 400, Total loss 0.032153\nStep 500, Total loss 0.032153\nStep 600, Total loss 0.032153\nStep 700, Total loss 0.032153\nStep 800, Total loss 0.032153\nStep 900, Total loss 0.032153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing is the audio regenerated through relu activation which is not at all close to the ground truth\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHow much memory do we save through this compression technique?\n\n\nMemory of audio file: 1232886 bytes\n\n\n\n\nNumber of parameters in the neural network are 198145\n\n\n\n\nMemory consumed by the neural network is 796343 bytes\n\n\n\n\nImage compression\nFor the sake of an example let’s take the classic cameraman photo\nThis is the architecture used for modelling the image, it contains 3 hidden layers and 256 neurons in each layer using the adam optimizer with learning rate 1e-4.\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\nNow let us train SIREN model on our dataset i.e the image. The loss and the current state of the Network is displayed for every 50th element.\n\n\nStep 0, Total loss 0.325293\nStep 50, Total loss 0.012300\nStep 100, Total loss 0.008443\nStep 150, Total loss 0.006012\nStep 200, Total loss 0.004077\nStep 250, Total loss 0.002673\nStep 300, Total loss 0.001977\nStep 350, Total loss 0.001566\nStep 400, Total loss 0.001294\nStep 450, Total loss 0.001094\nStep 500, Total loss 0.000939\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is an animation showing the learning the process of this image over SIREN:\nLet’s see how ReLU works in this setting. This is the architecture for the Neural Network:\n\n\nNetwork(\n  (net): Sequential(\n    (0): ReLULayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.733969\nStep 50, Total loss 0.077224\nStep 100, Total loss 0.064925\nStep 150, Total loss 0.057928\nStep 200, Total loss 0.052542\nStep 250, Total loss 0.048083\nStep 300, Total loss 0.044299\nStep 350, Total loss 0.041403\nStep 400, Total loss 0.039117\nStep 450, Total loss 0.037259\nStep 500, Total loss 0.035712\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see the learning pocess of ReLU based Neural Net on this image:\nNote that the laplacian of the image generated by ReLU activation is constant (or you can say zero) because the second derivative of the ReLU function is zero, because it is a linear function. Hence, it does not perform well for transmitting signals.\nAnother takeaway for us was that the task of these kinds of Neural nets is to actually overfit over the data. By overfitting the data, it basically memorised the outputs for the given inputs. In generic Machine learning applications sinusoidal activation functions would not work at all.\nThis method is approximately 12 times better than the regular way of transmitting images. But the catch is that training the neural network for a single image of decent quality takes an hour on fairly strong computers. This renders this message of compression very unpractical. Hence, it is not used for real world data compression yet. If somehow the time required for training could be lowered, it would be quite usable in real life applications."
  },
  {
    "objectID": "docs/posts/optim/optim.html",
    "href": "docs/posts/optim/optim.html",
    "title": "Optimizers in Machine Learning",
    "section": "",
    "text": "In the optimization world, optimizers play a crucial role in improving machine learning models. They adjust the model’s settings to reduce the prediction errors, which are measured by the loss function.\nOptimizers come in two main types:\nFirst-Order Optimizers: These use partial derivative values to guide the optimizer. They indicate if the loss is increasing or decreasing at a particular point, like a compass guiding the optimizer to minimize the loss.\nSecond-Order Optimizers: These go further by considering both first-order and second-order derivatives. Delving deeper into the curvature of the loss function, they can, in some cases, achieve more efficient optimization. This provides deeper insights into the shape of the loss function, making optimization more efficient.\nChoosing the right optimizer can make a significant difference in the training speed and performance of machine learning models. For example, Gradient Descent is a widely used first-order optimizer, while more complex tasks may require second-order optimizers like Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\nSelecting the right optimizer is crucial in machine learning model development, as each one has its strengths and weaknesses.\nA non-exhaustive list of first-order optimizers are:\nIn this blog, we shall be limiting our discussion to the following optimizers:"
  },
  {
    "objectID": "docs/posts/optim/optim.html#gradient-descent-vanilla-gradient-descent",
    "href": "docs/posts/optim/optim.html#gradient-descent-vanilla-gradient-descent",
    "title": "Optimizers in Machine Learning",
    "section": "Gradient Descent (Vanilla Gradient Descent)",
    "text": "Gradient Descent (Vanilla Gradient Descent)\nThis optimizer takes steps proportional to the negative gradient of the function at the current point. Think of it as trying to find the lowest point in a valley by walking in the direction where the slope is steepest.\nIn the vanilla gradient descent, all the training data is used to compute the gradient of the loss function w.r.t the parameters. So if we have 10000 datapoints, all of them undergo a forwards pass in a neural network and then the loss is computed. The loss is then backpropagated through the network to compute the gradients w.r.t the parameters. We will have 10000 forward passes and one backward pass in this case.\nAdvantages:\n\nDeterministic Updates: Since the gradients are computed using all the training data, the updates to the parameters are deterministic. This means that the updates are not dependent on the order of the training data. This is a very desirable property as we want the model to converge to the same minima regardless of the order of the training data. This also results in a smooth convergence.\nSuitable for small datasets: Since this uses all the datapoints to calculate the gradients, this leads to a more stable convergence.\n\nDisadvantages:\n\nResource Intensive: Since we use all the datapoints to calculate the gradients, all the values have to be stored in the memory. This can be a problem for large datasets."
  },
  {
    "objectID": "docs/posts/optim/optim.html#stochastic-gradient-descent",
    "href": "docs/posts/optim/optim.html#stochastic-gradient-descent",
    "title": "Optimizers in Machine Learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nInstead of the entire dataset, SGD uses a single data point at each iteration to move towards the minima. So we will have 10000 forward passes and 10000 backward passes. This is because the gradients are calculated using only one datapoint, and hence the direction of the gradient is not very accurate. This leads to a very noisy convergence as shown.\nAdvantages:\n\nLess Resource Intensive: Since we use only one datapoint to calculate the gradients, we need to store only one datapoint in the memory. This is very useful for large datasets.\nFaster Convergence: More frequent updates lead to quicker convergence as compared to vanilla gradient descent.\n\nDisadvantages:\n\nNoisy Convergence: Since we use only one datapoint to calculate the gradients, the direction of the gradient is not very accurate. This leads to a very noisy convergence.\nNon-Deterministic Updates: Since the gradients are calculated using only one datapoint, the updates to the parameters are not deterministic. This means that the updates are dependent on the order of the training data. This is not a very desirable property as we want the model to converge to the same minima regardless of the order of the training data. Sometimes it can also lead to non-convergence of the model."
  },
  {
    "objectID": "docs/posts/optim/optim.html#mini-batch-gradient-descent",
    "href": "docs/posts/optim/optim.html#mini-batch-gradient-descent",
    "title": "Optimizers in Machine Learning",
    "section": "Mini Batch Gradient Descent",
    "text": "Mini Batch Gradient Descent\nA middle ground between SGD and Vanilla Gradient Descent, it uses batches of data points for each update. In Mini Batch Gradient Descent, we use a batch of datapoints to calculate the gradients. So for each epoch, we will have 10000/batch size forward passes and 1 backward pass.\nAdvantages:\n\nLess Resource Intensive: Since we use only a batch of datapoints to calculate the gradients, we need to store only a batch of datapoints in the memory. This is very useful for large datasets.\nFaster Convergence: More frequent updates lead to quicker convergence as compared to vanilla gradient descent.\n\n\nAs compared to SGD, it has a more stable convergence as shown.\n\nIts Disadvantages:\n\nNoisy Convergence: Since we use only a batch of datapoints to calculate the gradients, the direction of the gradient is not very accurate. This leads to a very noisy convergence as shown in Fig 1.\n\nWe observed that SGD and Mini Batch Gradient Descent have a noisy convergence as compared to Vanilla Gradient Descent. This is because the direction of the gradient is not very accurate.\nWhat if we want a faster convergence? One way is to reduce the observed sharp edges in SGD and Mini Batch SGD. This can be done by using a momentum term. This is where Momentum comes in."
  },
  {
    "objectID": "docs/posts/optim/optim.html#momentum",
    "href": "docs/posts/optim/optim.html#momentum",
    "title": "Optimizers in Machine Learning",
    "section": "Momentum",
    "text": "Momentum\nMomentum’s Intuition is that it confidently speeds up the convergence by accumulating the gradient of the past steps. It is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Fig 1. It does this by adding a fraction of the update vector of the past time step to the current update vector. This fraction is called the momentum coefficient and is usually set to 0.9 or a similar value.\nIt’s akin to a ball rolling down a hill, gathering speed – unlike standard SGD, which feels more step-by-step.\nThis also gives us the intuition that if we have a very noisy convergence, we can use momentum to dampen the oscillations and get a smoother convergence. But it’s not all good. Momentum can also lead to overshooting of the minima and lots of to and fro oscillations.\nAnother question that can arise is if the term momentum is actually related to physics? The answer is yes. The momentum term is actually related to the momentum term in physics.\nInertia is the property of an object to resist changes in its state of motion. In optimization: Inertia is the property of an object to resist changes in its state of motion. The learning rate acts as the acceleration here and the force is the gradient. So we can see that the momentum term in SGD is actually related to the momentum term in physics.\n\\[\nv_{t+1} = \\mu \\cdot v_t - \\alpha \\cdot \\nabla J(\\theta_t)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t + v_{t+1}\n\\]"
  },
  {
    "objectID": "docs/posts/optim/optim.html#nesterov-accelerated-gradient",
    "href": "docs/posts/optim/optim.html#nesterov-accelerated-gradient",
    "title": "Optimizers in Machine Learning",
    "section": "Nesterov Accelerated Gradient:",
    "text": "Nesterov Accelerated Gradient:\nWhile momentum only takes into account the previous gradients, NAG looks ahead and corrects direction, leading to better adjustements in the steps. The Math and the intuition behind it: In the regular momentum update, we first compute the gradient at the current position and then take a big jump in the direction of the accumulated gradient (momentum). For NAG instead of calculating the gradient at the current position, we calculate the gradient after the momentum update, this is the look ahead feature and gives a better approximation to the gradient in the next step.\nInitialization:\n\nSet the initial parameters: \\(\\theta_0\\)\nSet the learning rate: \\(\\eta\\)\nSet the momentum parameter: \\(\\mu\\)\n\nNesterov Update Rule:\n\nCompute the gradient of the loss function at the predicted future parameter values:\n\n\\[\n\\nabla L(\\theta_t + \\mu v_t)\n\\]\n\nUpdate the velocity vector:\n\n\\[\nv_{t+1} = \\mu \\cdot v_t - \\eta \\cdot \\nabla L(\\theta_t + \\mu v_t)\n\\]\n\nUpdate the parameters using the velocity:\n\n\\[\n\\theta_{t+1} = \\theta_t + v_{t+1}\n\\]\nHere,\n\n\\(\\theta_t\\) represents the current parameter values at iteration (t).\n\\(\\eta\\) is the learning rate, controlling the step size.\n\\(\\mu\\) is the momentum parameter, which determines how much of the previous velocity to keep.\n\nNesterov Accelerated Gradient has been shown to converge faster than the standard gradient descent in many cases, making it a popular choice for optimization in deep learning and other machine learning applications.\nImagine hiking down a foggy mountain trail to reach a base camp quickly and safely. In this challenging setting, Standard Momentum is like a hiker who can only see the path directly in front of them due to the fog. They make decisions based on what’s immediately visible, risking overshooting or getting stuck when the path changes unexpectedly.\nIn contrast, NAG is like a hiker who can anticipate the path slightly ahead despite the fog. They periodically assess the terrain in advance, enabling more informed choices and efficient progress.\nAdvantages:\n\nOvershooting Reduction: Consider a scenario where the current gradient points in a certain direction, but due to noise or complex curvature of the loss landscape, it may not be the best direction to move in the long term. Standard Momentum would keep accumulating velocity in the current direction, potentially overshooting the optimal solution. NAG anticipates that the parameters will move in the direction of the lookahead point, which allows it to “course-correct” by reducing the accumulated velocity in the original direction. This anticipatory adjustment helps in reducing overshooting.\nNoise Handling: When gradients are noisy, they can cause erratic updates in standard Momentum. The lookahead mechanism of NAG dampens the impact of noisy gradients. By considering the gradient at the lookahead point, NAG effectively filters out some of the noise, resulting in more stable and reliable updates.\n\nDisadvantages:\n\nMore computationally complex: It requires an additional gradient computation at each iteration, which can be expensive for large models. Additional hyperparameter to tune: The momentum parameter needs to be tuned in addition to the learning rate \\(\\eta\\)."
  },
  {
    "objectID": "docs/posts/optim/optim.html#adagrad",
    "href": "docs/posts/optim/optim.html#adagrad",
    "title": "Optimizers in Machine Learning",
    "section": "Adagrad",
    "text": "Adagrad\nAdagrad is an optimization algorithm that adapts the learning rate for each parameter during training, allowing for more effective updates. It is also observed that Adagrad is particularly good for sparse distributions of data.\nMathematical Initialization:\n\nSet the initial parameters: \\(\\theta_0\\)\nDefine the learning rate: \\(\\eta\\)\n\nAdagrad Update Rule:\n\nCompute the gradient of the loss function: \\(\\nabla L(\\theta_t)\\)\nAdapt the learning rate for each parameter based on the historical gradient information:\n\n\\[\n\\text{Adapted Learning Rate for Parameter, } i: \\frac{\\eta}{\\sqrt{G_{ii} +\\epsilon}}\n\\]\nHere, \\(G_{ii}\\) represents the sum of the squares of historical gradients for parameter i, and \\(\\epsilon\\) is a small constant to prevent division by zero.\n\nUpdate the parameters:\n\n\\[\n\\theta_{t+1} = \\theta_t - i \\cdot \\nabla L(\\theta_t)\n\\]\nAdvantages:\n\nLearning Rate Adaptation: Adagrad dynamically adjusts the learning rate for each parameter, scaling it inversely with the square root of the sum of past squared gradients. This adaptability allows Adagrad to allocate larger learning rates to parameters with infrequent updates and smaller learning rates to parameters with frequent updates. As a result, it can effectively handle different convergence rates across parameters.\nNo Manual Learning Rate Tuning: Adagrad eliminates the need to manually tune the learning rate, which can be challenging to get right in practice.\n\nDisadvantages:\n\nAccumulation of Gradients: The denominator starts accumulating squares, which can lead to a very small learning rate and effectively stop the learning process.\nMemory Intensive: Adagrad accumulates the squares of the gradients in the denominator, which can become very large for large models and lead to memory issues.\n\nNote: Although it can correct learning rate according to the need to a good extent, a very poor choice of learning rate can still lead to a bad convergence."
  },
  {
    "objectID": "docs/posts/optim/optim.html#rmsprop",
    "href": "docs/posts/optim/optim.html#rmsprop",
    "title": "Optimizers in Machine Learning",
    "section": "RMSProp",
    "text": "RMSProp\nRMSProp is an optimization algorithm that builds upon the Adagrad method by addressing some of its limitations. It adapts the learning rate for each parameter during training, offering more stable and efficient updates.\nInitialization:\n\nSet the initial parameters: \\(\\theta_0\\)\nDefine the learning rate: \\(\\eta\\)\nSpecify the decay parameter: \\(\\rho\\) (typically close to 1)\n\nRMSProp Update Rule:\n\nCompute the gradient of the loss function: \\(\\nabla L(\\theta_t)\\)\nCalculate the exponentially moving average of the squared gradients for each parameter:\n\n\\[\nE[G_{ii}] = \\rho E[G_{ii}] + (1 - \\rho)(\\nabla L(\\theta_t))^2\n\\]\nHere, \\(E[G_{ii}]\\) represents the exponentially moving average of the squared gradients for parameter i.\n\nAdapt the learning rate for each parameter:\n\n\\[\n\\text{Adapted Learning Rate for Parameter, } i: \\frac{\\eta}{\\sqrt{E[G_{ii}] + \\epsilon}}\n\\]\n\nUpdate the parameters:\n\n\\[\n\\theta_{t+1} = \\theta_t - i \\cdot \\nabla L(\\theta_t)\n\\]\nAdvantages:\n\nStability: RMSProp addresses the issue of rapidly decreasing learning rates by introducing an exponentially moving average for the squared gradients. This results in a more stable and adaptive learning rate.\nNo Manual Learning Rate Tuning: Similar to Adagrad, RMSProp eliminates the need for manual tuning of the learning rate, making it a convenient choice for practical applications.\n\nDisadvantages:\n\nMemory Intensive: RMSProp still accumulates historical gradient information, which can be memory-intensive for large models, though to a lesser extent than Adagrad.\nHyperparameter Tuning: RMSProp introduces the decay parameter , which needs to be tuned along with the learning rate. Finding the right values for these hyperparameters can be a trial-and-error process.\n\nIn summary, RMSProp overcomes some of the issues of Adagrad, particularly the problem of a decreasing learning rate. It provides more stable training and is often used in deep learning models. However, it still requires careful tuning of hyperparameters and can be memory-intensive for very large models."
  },
  {
    "objectID": "docs/posts/optim/optim.html#adam-adaptive-moment-estimation",
    "href": "docs/posts/optim/optim.html#adam-adaptive-moment-estimation",
    "title": "Optimizers in Machine Learning",
    "section": "Adam (Adaptive Moment Estimation)",
    "text": "Adam (Adaptive Moment Estimation)\nAdam is a popular optimization algorithm that combines elements of both RMSProp and momentum. It adapts the learning rate and keeps track of past gradients’ moving averages, providing efficient and stable updates.\nInitialization:\n\nSet the initial parameters: \\(\\theta_0\\)\nDefine the learning rate: \\(\\eta\\)\nSpecify the decay parameters: \\(\\beta_1\\) (typically close to 1) and \\(\\beta_2\\) (typically close to 1)\n\nAdam Update Rule:\n\nCompute the gradient of the loss function: \\(\\nabla L(\\theta_t)\\)\nCalculate the first and second moments (moving averages) of the gradients for each parameter:\n\n\\[\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(\\theta_t)\n\\] \\[\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla L(\\theta_t))^2\n\\]\nHere, \\(m_t\\) and \\(v_t\\) represent the first and second moments of the gradients, respectively.\n\nCorrect the moments for bias:\n\n\\[\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n\\] \\[\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n\\]\n\nAdapt the learning rate for each parameter:\n\n\\[\n\\text{Adapted Learning Rate for Parameter, } i: \\frac{\\eta}{(\\hat{v}_t)^\\frac{1}{2} + \\epsilon}\n\\]\nHere, \\(\\epsilon\\) is a small constant to prevent division by zero.\n\nUpdate the parameters:\n\n\\[\n\\theta_{t+1} = \\theta_t - i \\cdot \\hat{m}_t\n\\]\nAdvantages:\n\nEfficiency: Adam combines the benefits of adaptive learning rates (like RMSProp) and momentum. It effectively handles different convergence rates for parameters and speeds up convergence.\nStability: The correction for bias in the moments ensures that the algorithm remains stable even during the initial iterations.\n\nDisadvantages:\n\nHyperparameter Sensitivity: Like RMSProp, Adam introduces decay parameters \\(\\beta_1\\) and \\(\\beta_2\\), which require tuning for optimal performance. Incorrect settings can affect convergence.\nMemory Usage: Adam accumulates moments for each parameter, which can lead to increased memory usage, especially in deep learning models.\n\nAdam is widely used in deep learning due to its efficiency and stability, but it does require careful tuning of its hyperparameters for different tasks and models."
  }
]