[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blogs",
    "section": "",
    "text": "Siren\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nVannsh Jani and Viraj Vekaria\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nOptimization algorithms\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nViraj Vekaria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "code/sgd_momentum.html",
    "href": "code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nTealdsfhlKSDfa"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Optimization algorithms",
    "section": "",
    "text": "Test test"
  },
  {
    "objectID": "posts/post-with-code/sgd_momentum.html",
    "href": "posts/post-with-code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "docs/posts/SIREN/siren.html",
    "href": "docs/posts/SIREN/siren.html",
    "title": "Siren",
    "section": "",
    "text": "Implicit Neural representations using sinusoidal activation\n\n\nAudio compression\nThe reason we use the sine function as an activation function is because it is infinitely differentiable where as for a function like relu, the double derivative becomes zero. Let’s try regenerating an audio file by mapping coordinates in the grid with amplitudes\nThe following is the ground truth audio\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe are using a neural network with 3 hidden layers and each layer consisting of 256 neurons and we are using mean squared error loss function and optimizing it using Adam with a learning rate of 0.0001. We are training the neural network for a total of 1000 steps and displaying every 100th result. As can be seen in the following figures as the number of steps increases both the graphs converge.\nDescription of neural network\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=1, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.025424\nStep 100, Total loss 0.002972\nStep 200, Total loss 0.001123\nStep 300, Total loss 0.000861\nStep 400, Total loss 0.000702\nStep 500, Total loss 0.000586\nStep 600, Total loss 0.000535\nStep 700, Total loss 0.000538\nStep 800, Total loss 0.000665\nStep 900, Total loss 0.000362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph the coordinates of the grid are squeezed to a single dimension (x-axis) and the corresponding model output is shown on the y-axis.\nFollowing is the audio regenerated using a neural network with sinusoidal activation\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThere is a little bit of noise in the above regenerated audio signal but overall has a good performance\nLet’s try using a different activation function such as relu and how well it can perform to regenerate audio and compare it with sinusoidal activation\n\n\nStep 0, Total loss 0.032153\nStep 100, Total loss 0.032153\nStep 200, Total loss 0.032153\nStep 300, Total loss 0.032153\nStep 400, Total loss 0.032153\nStep 500, Total loss 0.032153\nStep 600, Total loss 0.032153\nStep 700, Total loss 0.032153\nStep 800, Total loss 0.032153\nStep 900, Total loss 0.032153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing is the audio regenerated through relu activation which is not at all close to the ground truth\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHow much memory do we save through this compression technique?\n\n\nMemory of audio file: 1232886 bytes\n\n\n\n\nNumber of parameters in the neural network are 198145\n\n\n\n\nMemory consumed by the neural network is 796343 bytes\n\n\n\n\nImage compression\nFor the sake of an example let’s take the classic cameraman photo\nThis is the architecture used for modelling the image, it contains 3 hidden layers and 256 neurons in each layer using the adam optimizer with learning rate 1e-4.\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\nNow let us train SIREN model on our dataset i.e the image. The loss and the current state of the Network is displayed for every 50th element.\n\n\nStep 0, Total loss 0.325293\nStep 50, Total loss 0.012300\nStep 100, Total loss 0.008443\nStep 150, Total loss 0.006012\nStep 200, Total loss 0.004077\nStep 250, Total loss 0.002673\nStep 300, Total loss 0.001977\nStep 350, Total loss 0.001566\nStep 400, Total loss 0.001294\nStep 450, Total loss 0.001094\nStep 500, Total loss 0.000939\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is an animation showing the learning the process of this image over SIREN:\nLet’s see how ReLU works in this setting. This is the architecture for the Neural Network:\n\n\nNetwork(\n  (net): Sequential(\n    (0): ReLULayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.733969\nStep 50, Total loss 0.077224\nStep 100, Total loss 0.064925\nStep 150, Total loss 0.057928\nStep 200, Total loss 0.052542\nStep 250, Total loss 0.048083\nStep 300, Total loss 0.044299\nStep 350, Total loss 0.041403\nStep 400, Total loss 0.039117\nStep 450, Total loss 0.037259\nStep 500, Total loss 0.035712\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see the learning pocess of ReLU based Neural Net on this image:\nNote that the laplacian of the image generated by ReLU activation is constant (or you can say zero) because the second derivative of the ReLU function is zero, because it is a linear function. Hence, it does not perform well for transmitting signals.\nAnother takeaway for us was that the task of these kinds of Neural nets is to actually overfit over the data. By overfitting the data, it basically memorised the outputs for the given inputs. In generic Machine learning applications sinusoidal activation functions would not work at all.\nThis method is approximately 12 times better than the regular way of transmitting images. But the catch is that training the neural network for a single image of decent quality takes an hour on fairly strong computers. This renders this message of compression very unpractical. Hence, it is not used for real world data compression yet. If somehow the time required for training could be lowered, it would be quite usable in real life applications."
  }
]