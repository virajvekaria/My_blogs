[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blogs",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nOptimization algorithms\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nViraj Vekaria\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "code/sgd_momentum.html",
    "href": "code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nTealdsfhlKSDfa"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Optimization algorithms",
    "section": "",
    "text": "Test test"
  },
  {
    "objectID": "posts/post-with-code/sgd_momentum.html",
    "href": "posts/post-with-code/sgd_momentum.html",
    "title": "My blogs",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Define the function\ndef function(x):\n    return torch.sin(x) + x**2\n\n# Convert the function to a PyTorch tensor\nx = torch.tensor([2.0], requires_grad=True)  # Starting point\n\n# Define the SGD optimizer with momentum\noptimizer = torch.optim.SGD([x], lr=0.1, momentum=0.9)\n\n# Create a figure and axis for the animation\nfig, ax = plt.subplots()\nax.set_xlim(-10, 10)\nax.set_ylim(-1, 6)\nline, = ax.plot([], [], 'ro', label='Optimization Path')\nfunc_line, = ax.plot([], [], label='Function: f(x) = sin(x) + 0.5x')\n\n# Initialization function for the animation\ndef init():\n    line.set_data([], [])\n    func_line.set_data([], [])\n    return line, func_line\n\n# Animation update function\nx_values = [2.0]\ndef animate(i):\n    optimizer.zero_grad()  # Clear gradients from previous iteration\n    \n    loss = function(x)  # Compute the loss\n    loss.backward()  # Compute gradients with respect to x\n    optimizer.step()  # Update x using the computed gradients\n    x_values.append(x.item())  # Store the current x value\n    \n    line.set_data(x_values, [function(torch.tensor(val)).item() for val in x_values])\n    func_line.set_data(np.linspace(-6, 6, 400), function(torch.tensor(np.linspace(-6, 6, 400))).detach().numpy())\n    return line, func_line\n\n# Create the animation\nnum_iterations = 100\nanim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations, interval=200, blit=True)\n\n# Show the animation\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Optimization using SGD with Momentum in PyTorch (Animation)')\nplt.legend()\nplt.grid(True)\nplt.show()"
  }
]